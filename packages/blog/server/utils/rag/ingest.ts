import { readdir, readFile } from 'node:fs/promises'
import { join } from 'node:path'
import { eq } from 'drizzle-orm'
import { chunkText, parseMarkdown, hashContent } from './chunker'
import { embedTexts } from '../ai/bedrock'
import { getAnthropicClient } from '../ai/anthropic'

const BLOG_CONTENT_PATH = 'content/2.blog'

// Prompt for generating contextual descriptions
const DOCUMENT_CONTEXT_PROMPT = `<document title="{title}" url="{url}">
{doc_content}
</document>`

const CHUNK_CONTEXT_PROMPT = `Here is a chunk from the document above:
<chunk>
{chunk_content}
</chunk>

Generate 1-2 sentences situating this chunk within the overall document for improving search retrieval. Focus on what topic this chunk covers and how it relates to the document's main theme. Answer only with the context, nothing else.`

export interface IngestResult {
  documentsProcessed: number
  documentsSkipped: number
  chunksCreated: number
  errors: string[]
}

/**
 * Generate contextual description for a chunk using Claude
 * Uses prompt caching to reduce cost when processing multiple chunks from same doc
 */
export async function generateContextualDescription(
  docTitle: string,
  docUrl: string,
  docContent: string,
  chunkContent: string
): Promise<string> {
  const client = getAnthropicClient()
  const config = useRuntimeConfig()

  const response = await client.messages.create({
    model: config.public.model_fast as string, // claude-haiku-4-5
    max_tokens: 200,
    temperature: 0,
    messages: [{
      role: 'user',
      content: [
        {
          type: 'text' as const,
          text: DOCUMENT_CONTEXT_PROMPT
            .replace('{title}', docTitle)
            .replace('{url}', docUrl)
            .replace('{doc_content}', docContent),
          cache_control: { type: 'ephemeral' as const } // Enable prompt caching
        },
        {
          type: 'text' as const,
          text: CHUNK_CONTEXT_PROMPT.replace('{chunk_content}', chunkContent)
        }
      ]
    }]
  })

  const textBlock = response.content.find(c => c.type === 'text')
  return textBlock?.type === 'text' ? textBlock.text : ''
}

/**
 * Ingest all blog posts into the RAG database
 */
export async function ingestBlogPosts(): Promise<IngestResult> {
  const db = useDrizzle()
  const result: IngestResult = {
    documentsProcessed: 0,
    documentsSkipped: 0,
    chunksCreated: 0,
    errors: []
  }

  // Get all markdown files from blog content
  const blogDir = join(process.cwd(), BLOG_CONTENT_PATH)
  let files: string[]

  try {
    files = (await readdir(blogDir)).filter(f => f.endsWith('.md'))
  } catch (error) {
    result.errors.push(`Failed to read blog directory: ${error}`)
    return result
  }

  for (const file of files) {
    try {
      const filePath = join(blogDir, file)
      const content = await readFile(filePath, 'utf-8')
      const contentHash = await hashContent(content)

      const parsed = parseMarkdown(content, file)
      const url = `/blog/${parsed.slug}`

      // Check if document exists and is unchanged
      const existingDoc = await db.query.documents.findFirst({
        where: (doc, { eq }) => eq(doc.slug, parsed.slug)
      })

      if (existingDoc && existingDoc.contentHash === contentHash) {
        result.documentsSkipped++
        continue
      }

      // Delete existing chunks if document changed
      if (existingDoc) {
        await db.delete(tables.documentChunks)
          .where(eq(tables.documentChunks.documentId, existingDoc.id))
        await db.delete(tables.documents)
          .where(eq(tables.documents.id, existingDoc.id))
      }

      // Create document record
      const insertedDocs = await db.insert(tables.documents).values({
        slug: parsed.slug,
        title: parsed.title,
        path: filePath,
        url,
        contentHash
      }).returning()

      const doc = insertedDocs[0]
      if (!doc) {
        throw new Error(`Failed to insert document: ${parsed.slug}`)
      }

      // Chunk the content
      const chunks = chunkText(parsed.content)

      // Process chunks: generate context, embed, store
      for (const chunk of chunks) {
        // Generate contextual description
        const contextualContent = await generateContextualDescription(
          parsed.title,
          url,
          parsed.content,
          chunk.content
        )

        // Generate embedding for combined content
        const textToEmbed = `${chunk.content}\n\nContext: ${contextualContent}`
        const embeddings = await embedTexts([textToEmbed])
        const embeddingResult = embeddings[0]

        if (!embeddingResult) {
          throw new Error(`Failed to generate embedding for chunk ${chunk.index}`)
        }

        // Store chunk (searchVector is auto-generated by PostgreSQL)
        await db.insert(tables.documentChunks).values({
          documentId: doc.id,
          chunkIndex: chunk.index,
          content: chunk.content,
          contextualContent,
          embedding: embeddingResult.embedding
        })

        result.chunksCreated++
      }

      result.documentsProcessed++
      console.log(`Ingested: ${parsed.title} (${chunks.length} chunks)`)
    } catch (error) {
      const errorMsg = `Failed to process ${file}: ${error}`
      result.errors.push(errorMsg)
      console.error(errorMsg)
    }
  }

  return result
}

/**
 * Ingest a single document by slug
 */
export async function ingestDocument(slug: string): Promise<IngestResult> {
  const result: IngestResult = {
    documentsProcessed: 0,
    documentsSkipped: 0,
    chunksCreated: 0,
    errors: []
  }

  const blogDir = join(process.cwd(), BLOG_CONTENT_PATH)
  let files: string[]

  try {
    files = (await readdir(blogDir)).filter(f => f.endsWith('.md'))
  } catch (error) {
    result.errors.push(`Failed to read blog directory: ${error}`)
    return result
  }

  // Find file matching slug
  const file = files.find((f) => {
    const parsed = parseMarkdown('', f)
    return parsed.slug === slug
  })

  if (!file) {
    result.errors.push(`Document not found: ${slug}`)
    return result
  }

  // For now, just re-run full ingestion (incremental by hash)
  // TODO: Implement single-document re-ingestion
  return ingestBlogPosts()
}
