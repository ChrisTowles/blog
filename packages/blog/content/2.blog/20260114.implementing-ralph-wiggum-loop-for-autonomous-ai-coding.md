---
draft: true
title: "Implementing a Ralph Wiggum Loop: The Secret is Session Markers"
description: "How I turned towles-tool into an autonomous AI coding system using session markers to preserve context across iterations - and the weekend of mistakes that got me there."
date: "2026-01-14"
badge:
  label: "Claude Code"

image:
  src: "/images/blog/20260114-1530-ralph-wiggum-loop.png"
  alt: "A developer's desk at 3am illuminated by multiple monitors showing endless loops of terminal output and code, an empty coffee mug and scattered energy drink cans nearby. One monitor displays a glowing circular arrow symbol representing infinite iteration, another shows JSONL session files being connected like constellation lines. Dramatic blue screen glow contrasts with warm desk lamp creating noir atmosphere, shallow depth of field focusing on a sticky note reading "504 iterations" with concern scrawled beside it. Cinematic realism, 8k photorealistic quality, cyberpunk undertones with teal and amber color grading, slightly overhead angle capturing the obsessive late-night debugging mood. But include ralph wiggum a the screen"
authors:
  - name: Chris Towles
    to: https://twitter.com/Chris_Towles
    avatar:
      src: /images/ctowles-profile-512x512.png
---

There's an AI coding pattern that lets you run seriously long-running agents - for hours at a time - that ship code while you sleep. It's called the **Ralph Wiggum loop**, named after the haplessly persistent Simpsons character.

I'd been tinkering with this pattern since Claude Code's early skill system - trying to run loops in interactive sessions, wrestling with state management, burning through context limits. Then Matt Pocock [published his approach](https://www.aihero.dev/tips-for-ai-coding-with-ralph-wiggum), and one thing clicked immediately:

**Use a bash shell. Not an interactive session.**

It sounds obvious in retrospect. Why was I trying to manage loops inside Claude Code's interactive session when I could just... run `claude` from a bash while loop? Queue up the work, let Claude figure out what to do next, repeat until done.

After a weekend of burning through tokens, migrating from pnpm to Bun (and back), and discovering the critical importance of session markers, I finally got it working. Here's what I learned.

## What is a Ralph Wiggum Loop?

The pattern is elegantly simple: a while loop that feeds Claude a prompt, Claude works on the code, and when Claude attempts to exit, a hook checks if work is truly done. If not, it re-feeds the same prompt. Each iteration carries forward all context - the modified files, git history, previous test results - allowing Claude to learn from its own output.

Matt Pocock describes it as "a keep-it-simple-stupid approach to AI coding" The official [Claude Code plugin ](https://github.com/anthropics/claude-code/blob/main/plugins/ralph-wiggum/README.md) but the pattern was created/found by [Geoffrey Huntley](https://ghuntley.com/ralph/). Its been all over x.com and reddit but how do you leverage it in your own projects?


## The Aha Moment: Queue, Don't Direct

Before Matt's video, I was thinking about this wrong. I'd start an interactive Claude Code session, add tasks to a state file, then try to run a loop *inside* the conversation. Complex prompt engineering. Hooks to detect exit conditions. State management within the session.

The insight from Matt's approach: **let Claude decide what to work on**. You're not micro-managing each step. You queue up tasks, give Claude access to a state file, and run the CLI in a bash loop. Each iteration, Claude reads the state, picks a task, does the work, marks it done, and exits. The loop restarts, Claude picks the next task.

It's mechanical. It's simple. It works overnight while you sleep.

One thing that made a huge difference: **task instructions in the system prompt**. towles-tool injects the current task and workflow rules into the system prompt via `--append-system-prompt`, not just the user message. This keeps Claude focused on the current task much longer and more successfully than putting instructions in the prompt itself. The system prompt is "always there" in a way that user messages aren't.

## The Token Burn Weekend (Or: Why I Have a File Named NEVER_AGAIN)

One December weekend, I set up what I thought was a reasonable experiment: let Claude work autonomously on a complex feature - personas, chatbots, skills, database schema changes. I ran the loop overnight.

I woke up to this in my state file:

```yaml
iteration: 10
max_iterations: 10  
```

**10 iterations.** I'd set `max_iterations: 10` and gone to sleep. The commit message from that night says it all: I'd burn through all my tokens and I'm on the $100 plan.

The state file got renamed to `.claude/ralph-loop.local.md-NEVER_AGAIN`.

The problem wasn't just the raw token count. It was that every single iteration started after the one before. Each iteration had to use the context from the previous iteration. Context is everything and even with compaction, Claude code is much smarter in the first 40% of the context than when its full and trying to compact it.


The [commit that changed everything on 2026-01-11](https://github.com/ChrisTowles/towles-tool/commit/ee6db37): `feat(ralph): auto-resume sessions per-task to prevent token burn`. That commit message tells the story. After the NEVER_AGAIN incident, I had to figure out how to preserve context between iterations but not fill up the context, find that sweet spot where Claude has enough context to be effective without burning through tokens unnecessarily.

## The Secret: Session Markers

So assigning Tasks to Claude code in a "headless" manner, aka bash loop is half the battle. How do you give Claude the right context for each task without starting from scratch every time? Usally its to write a plan.md file right? but even with a half page of markdown it still has alot of intent to figure out. So thought about how to preserve context more effectively. My solution: **session markers**.

Boris Cherny had tweeted about features and even called out that there was already a way to resume sessions in Claude Code.

Claude Code stores conversation history in JSONL files at `~/.claude/projects/`. Each file represents a session. If you can find the right session, you can resume from it with `--resume <session-id>`, and Claude picks up exactly where it left off.

The insight that changed everything: **generate a random marker, have Claude output it during research, then search for that marker to find the session**.


Something like:

```typescript
// From marker.ts - the core of the system
export const MARKER_PREFIX = "RALPH_MARKER_";

export function generateMarker(): string {
  const chars = "abcdefghijklmnopqrstuvwxyz0123456789";
  let result = "";
  for (let i = 0; i < 8; i++) {
    result += chars.charAt(Math.floor(Math.random() * chars.length));
  }
  return result;
}

export async function findSessionByMarker(marker: string): Promise<string | null> {
  const projectDir = getProjectDir();
  // ... search all .jsonl files for the marker
  // return the session ID (filename without .jsonl)
}
```

So it something like this:

```bash
# 1. Generate a marker
tt ralph marker create
# Output: RALPH_MARKER_abc12345

# 2. Do your research with Claude, telling it to output the marker
# "When you've gathered all the context needed, output: RALPH_MARKER_abc12345"

# 3. Add tasks with the marker
tt ralph task add "Implement the feature" --findMarker RALPH_MARKER_abc12345
# âœ“ Added task #1 with session: 7a9f3b2c...

# 4. Run ralph - it automatically forks from the research session!
tt ralph run
```

Each task now carries its own `sessionId`. When ralph runs that task, it forks from that session. Claude starts with all the codebase knowledge from the research phase - no more burning tokens re-discovering the same files, doing redundant work, or losing context between iterations.

## The Implementation

After all the migrations and false starts, here's what `towles-tool` looks like now: https://github.com/ChrisTowles/towles-tool


The combination of session markers plus its a Claude code plugin with commands and skills to plan work which knows how to create tasks with the correct session context is what makes Ralph loops truly autonomous.

I'll post a follow-up blog post on usage but my results so far are promising! 



## Real World: Reviewing 37 Blog Posts

My first real use of the system: [reviewing every blog post in my repository](https://github.com/ChrisTowles/blog/pull/162).

I created 37 tasks - one for each post - ran ralph overnight, and woke up to:
- Grammar fixes
- Improved clarity
- Consistent formatting
- All committed individually with meaningful messages

I let each blog post be reviewed autonomously by Ralph overnight, and when I woke up, all 37 posts had been improved with minimal manual intervention. I've been using it to plan out more of my work autonomously as well. I plan while claude works, i setup work tree's but honestly Claude code is so fast my team is better planning with it, rather than fighting git worktrees like [i did here!](https://github.com/ChrisTowles/blog/blob/main/scripts/worktree.ts)

## Is this Real Life?

Surely someone has already thought of this before? I did a quick search and couldn't find any existing implementations that combined session markers with autonomous task execution in Claude Code, so it seems like this approach is relatively novel.

I started to hack this into a my "towles-tool" CLI. Creating both a claude plugin and CLI commands to manage session markers and autonomous task execution.

Its hard to mess with session markers compared to without, I've used it to do hard tasks like converting this blog's AI chat from API driven to websockets. I can promise you it ralph with session markers worked first try morning after and the previoud 2 attempts failed. And that was once with me parterning and once with it solo that burned through my token quota.

So lets get some more eyes on this! Let me know what you think.

![Towles-tool ralph loop results](/images/blog/20260114-ralph-loop-terminal.png)

## Lessons Learned

1. **Context is everything**. A loop without session preservation is just expensive repetition.

2. **The marker trick is the secret sauce**. Generate a random string, have Claude output it, search for it later. Simple but powerful.

3. **Start with HITL, then go AFK**. Matt's advice: run interactively first, verify it's working, then let it run autonomously.

4. **Set conservative iteration limits**. A 20-iteration loop on a large codebase is plenty, its more work than can do in a single night and its likely more than you can review in one sitting.

5. **Token efficiency compounds**. do the work once, save the session, and fork from it for all subsequent tasks. This avoids redundant context rebuilding and saves both time and API credits.

## Insights

One of the features i built into towles-tool is the ability to visualize the where i was spending my tokens quota.

I had tried a flamegraph but a tree map ended up being more informative.

![Token usage treemap visualization showing Claude session costs](/images/blog/20260114-token-usage-treemap.png)

## What's Next

Gather feedback from other and my own usage of the tool to refine the experience and improve token efficiency.

## Try It

The code is open source:
- **towles-tool**: [github.com/ChrisTowles/towles-tool](https://github.com/ChrisTowles/towles-tool)

Install with `npm install -g @towles/tool` and start with `tt ralph --help`.

it has other commands and skills as well, (improve, normal planning via interview, Journaling in markdown too!)

And install the Claude code plugin.

```bash
claude plugin marketplace add ChrisTowles/towles-tool
claude plugin install tt@towles-tool --scope user
```

The Ralph Wiggum loop isn't magic. It's a while loop. Claude Code is the magic. But add some context preservation and with session forking and it gives you even more leverage. Get those right, and you can ship while you sleep but not blow through your token quota!

---

*Thanks to [Matt Pocock](https://www.aihero.dev/tips-for-ai-coding-with-ralph-wiggum) for writing about Ralph Wiggum loops and inspiring this implementation.*
