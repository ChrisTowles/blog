---
title: Voice to System
description: "TODO"
date: 2025-07-05
image:
  src: /images/blog/TODO-voice-to-text.png
  alt: "TODO"

  # generated: with fooocus Styles 'Fooocus Enhance', 'SAI Fantasy Art', 'SAI Comic Book'
  # prompt: Create an image that visually represents the core message of adopting software engineering best practices and ITIL for a better work-life balance. The scene should feature a modern developer’s workspace: a tidy desk with a laptop open to a code editor (like VS Code), surrounded by elements symbolizing organization, automation, and calm—such as checklists, flowcharts, and a cup of coffee. The atmosphere should convey productivity, reduced stress, and harmony between technology and personal well-being. No text or logos.

authors:
  - name: Chris Towles
    to: https://twitter.com/Chris_Towles
    avatar:
      src: /images/ctowles-profile-512x512.png

badge:
  label: AI Tools
---

How one man made me realize i wasn't using AI enough at my work where i'm the Princial Arcectic Cloud Services AI at  GE Aerspace.


So while I've had my mind blown all weekend after watching Anthropics clod code video. They really made me consider like how much we let the tooling limit with the agent can do in our repos because. Literally a week and a half ago I posted how I was using the. Voice to speech integration in VS code. To help, use AI in VS code better, period. But that actually was a hindrance. 


One of the most amazing things about the video about Claude code is it removes all the distractions. These models get better. Anything you do betting on the model, like any tool you try to give it, is actually likely to be harming it. To find. A local maximum. With Cloud code, the interface is trying to do is the same tool. We've been using for the last 50 years. His iteration going through ed editor looks like a standard batch statement. So by giving the model the same way, we would try to fix it at the terminal with the only interface we've ever found of works. So on my voice dictation, I was not. I was still typing in the clock code. Period. I wasn't just entering. The commands I wanted Claude to do with my voice. I was typing them out. When I was adding markdown, I could use the VS Code voice integration with the editor. However, VS Code's voice integration doesn't work the same way outside of the editor window. When I'm in the terminal and I start typing it or start dictating to it, I can't have it dictate. Instead I can dictate to chat, GPT, or whatever model I'm using for copilot. But it interprets that command and I can get that to put that in the terminal. I don't want that layer just like the anthropics COD code talk. I want that to be is I don't I don't want VS code between it. So like to his comment and this was like I blew my mind. Um, I need to get system note after takes it back after. I really struggled with getting. VS code audio settings. I spend probably an hour messing around with like, oh, there's a plugin for VS code, maybe it'll help. They'll do dictation better than that or. But what I should do is like I just, I don't want to have to have VS code. I just want to talk to the terminal window, right? So how do you do that in Linux? Well, you need to as a OS level dictation. What was crazy was when I was trying to find the best dictation to use on a Linux terminal, I actually was taken back to the same man who gave that talk. On a GitHub reply saying that the way he's built most of cloud of code is by using OS level dictation in a terminal, right. No VS code in between, which just yeah, he's he's really really right and I am aware that he recently took a new job for cursor, so good luck to them, but I think he's on to something. 

