---
title: Voice to System
description: "How watching Anthropic's Claude Code lead work made me realize I wasn't using AI enough, and the journey to OS-level voice dictation for better terminal workflows."
date: 2025-07-05
image:
  src: /images/blog/TODO-voice-to-text.png
  alt: "Modern developer workspace with clean desk, laptop, and organized tools representing productive AI workflow"

  # generated: with fooocus Styles 'Fooocus Enhance', 'SAI Fantasy Art', 'SAI Comic Book'
  # prompt: Create an image that visually represents the core message of adopting software engineering best practices and ITIL for a better work-life balance. The scene should feature a modern developer’s workspace: a tidy desk with a laptop open to a code editor (like VS Code), surrounded by elements symbolizing organization, automation, and calm—such as checklists, flowcharts, and a cup of coffee. The atmosphere should convey productivity, reduced stress, and harmony between technology and personal well-being. No text or logos.

authors:
  - name: Chris Towles
    to: https://twitter.com/Chris_Towles
    avatar:
      src: /images/ctowles-profile-512x512.png

badge:
  label: AI Tools
---

How one man made me realize I wasn't using AI enough at my work where I'm the Principal Architect for Cloud Services AI at GE Aerospace.


What do you want in a day? On Friday, I watched a video by Anthropic's Claude Code lead developer. I knew this man was recently in the news. I knew he had jumped to the Chief Architect @anysphere. Which owns Curor. which is really relevant with all the talent shuffling going on between OpenAI, Meta, and everyone else right now. I'd seen posts by other refrencing him but never one of his talks. But this video changed everything. It make me realize. That even though i've be trying to increase my usage of AI. it was wasn't thinking big enough. 

https://youtu.be/Lue8K2jqfKk?si=rF5CoJAZWiFbAFvN


In this video he really laid out how he's using Claude, and I've really been impressed with Claude Code the last few weeks. But after thinking about https://x.com/mattpocockuk/status/1940693600737841272 video about how he's using AI agents to work by them selves because he's invested in his CICD and Tests. 

Back to watching Boris Cherny talk about why claude is built the way it is. That by removing everything between the model and the files and system it self. The better at everything it gets.  No more leaky abstraction between the model and the lower level system, and files.  But in showing how the reason I use the command line tools almost every second i'm on a computer because its still the most powerful.

That i use the terminal and my autocomplete, colored output, aliases and dotfiles is becase thats the best abstraction we've had since ed in 19XX.

And aligned that the less cruches you put between the model and your cli. The more powerful it became as the model improved. Just like he refrenceds in http://www.incompleteideas.net/IncIdeas/BitterLesson.html. 
I'd read it before after haven been refrenced by andrey kapathy. 

he used those tools, I realized that Claude Code is so close to the actual files. The way I would edit files, the way I would work if I was doing everything really high level, but it lets me do those things on my machine. Everything in between me and the files was really an abstraction - a limiting abstraction. Every tool added between those files, even VS Code, as good as it is, creates distance. This kind of blew my brother's mind as well. We've been talking about it for the last 24 hours, and then while working on this on my machine during my free time after taking the kids to the pool. 

I am aware that he recently took a new job at Cursor, so good luck to them on that Hire because I think he's on to something. 

So while I've had my mind blown all weekend after watching Anthropic's Claude Code video, they really made me consider how much we let the tooling limit what the agent can do in our repos. Literally a week and a half ago I posted how I was using the voice-to-speech integration in VS Code to help use AI in VS Code better. But that actually was a hindrance. 


One of the most amazing things about the video about Claude Code is it removes all the distractions. These models get better when you don't constrain them. Anything you do betting on the model, like any tool you try to give it, is actually likely to be harming it by finding a local maximum. With Claude Code, the interface is trying to use the same tool we've been using for the last 50 years. His iteration going through the editor looks like a standard batch statement. So by giving the model the same way we would try to fix it at the terminal - with the only interface we've ever found that works - we get better results. So on my voice dictation, I was not being efficient. I was still typing in Claude Code. I wasn't just entering the commands I wanted Claude to do with my voice. I was typing them out. When I was adding markdown, I could use the VS Code voice integration with the editor.


However, VS Code's voice integration doesn't work the same way outside of the editor window. When I'm in the terminal and I start typing or start dictating to it, I can't have it dictate. Instead I can dictate to ChatGPT or whatever model I'm using for copilot. But it interprets that command and I can get that to put that in the terminal. I don't want that layer, just like the Anthropic's Claude Code talk. I don't want VS Code between it. So like his comment - and this was what blew my mind - I need to get system-level dictation. I really struggled with getting VS Code audio settings. I spent probably an hour messing around with plugins for VS Code, thinking maybe they'll help with dictation better. But what I should do is just not have VS Code. I just want to talk to the terminal window, right?

So how do you do that in Linux? Well, you need OS-level dictation. What was crazy was when I was trying to find the best dictation to use on a Linux terminal, I actually was taken back to the same man, [Boris Cherny](https://github.com/bcherny), who gave that talk. On a [GitHub issue reply](https://github.com/anthropics/claude-code/issues/154#issuecomment-2856756037) saying that the way he's built most of Claude Code is by using OS-level dictation in a terminal. Mind blown! No VS Code in between, which just validates that he's really right. The idea that by trying to get closer to the concept of nothing between the files and the model brings me back to an unrelated topic and concept aligned towards nothing between you and the model. 


