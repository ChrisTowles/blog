---
title: Claude Code Tips
description: ""
date: 2025-07-20
image:
  src: /images/blog/todo-place-holder-image.png
  alt: ""

  # generated: with fooocus Styles 'Fooocus Enhance', 'SAI Fantasy Art', 'SAI Comic Book'
  # prompt:

authors:
  - name: Chris Towles
    to: https://twitter.com/Chris_Towles
    avatar:
      src: /images/ctowles-profile-512x512.png

badge:
  label: "Claude Code"
---

## Most important

- Claude is a powerful AI tool that can help with a variety of tasks, including code generation, text summarization, and more. but before you allowed it to be commited to a repo. Understand IT EVERY LINE!!!!. 

I mean that several levels, first ACUTALLY READ IT!. I don't mean skim it, I mean read it. Read the code, read the document update. Because not doing so is like \<github-copilot-suggestion> ..... 


### My point proven!

I want to pause here and tell you that `Github Copilot` just suggested that I write:

```md
 giving a child a loaded gun and not teaching them how to use it. 

 ```

It's the perfect example of the point I'm trying to make. Its almost right, but completely wrong. The LLM randomness missed just enough to make it not work. What that means when writing text is that you. You say something you might not normally say. Could be a joke, could be a metaphor, but it doesn't work. It doesn't make sense. It doesn't fit the context.

And that's the problem with LLMs. They don't understand context. They don't understand the nuances of language. They don't understand the implications of what they're saying.

## IN CODE ITS A MILLTION TIMES WORSE!

When you're writing code, you have to be precise. You have to be exact. You have to understand the implications of what you're writing. You can't just throw something out there and hope it works. compiling and linting doesn't mean it's not broken. You have to know what you're doing.

The more code you change, the more you have to understand the context. The more you have to understand the implications of what you're writing. The more you have to understand the nuances of language.  
IN CODE ITS A MILLTION TIMES WORSE! LLMs introduce the worse type of bugs. The type that take a lot of time to find. because they mostly work. But when they don't work, they can be very subtle and hard to track down.

So my new personal rule is to never allow an LLM to be commited to a repo without reading every line of code. I mean every line. I mean every line of the document update. I mean every line of the code that was changed. I mean every line of the code that was added. I mean every line of the code that was removed.  And the reason is the time saved using LLM is lots of time lost when you have to track down a bug that was introduced by an LLM. So small commits, small enough that you'll actually read it and understand it.

## Conclusion

Claude Code and other AI tools are incredibly powerful productivity enhancers, but they come with a critical caveat: you must remain the expert in charge. The time saved by LLM-generated code can quickly be lost tracking down subtle bugs that "mostly work" but fail in edge cases. 

The key is treating AI as a smart assistant, not a replacement for understanding. Read every line it generates. Understand the context and implications. Make small, reviewable commits. The moment you stop being the expert who validates the AI's work is the moment you've traded short-term speed for long-term technical debt and debugging nightmares.

Use AI tools wisely â€“ let them accelerate your work, but never let them replace your judgment.

