---
# theme: seriph

theme: default
background: https://images.unsplash.com/photo-1516849841032-87cbac4d88f7?q=80&w=2070
highlighter: shiki

title: "The No-Brainer Upgrade"

info: |
  ## Claude Sonnet 4.5 vs 3.5
  Same price, dramatically better performance
class: text-center
drawings:
  persist: false
transition: slide-left
mdc: true
duration: 10min
---

<div class="absolute inset-0 bg-gradient-to-br from-black/60 to-black/40" />

<div class="relative z-10">

# üöÄ The No-Brainer Upgrade

## From Good to Great: AI Model Evolution

<div class="text-sm opacity-75 mt-4">
Why upgrading your AI model is like upgrading from a prop plane to a rocket
</div>

</div>

<!--
Opening hook: We've been using AI for over a year. Everyone says "AI is moving fast" - but let me show you exactly HOW fast with concrete numbers. This isn't hype; these are measurable, verified improvements that translate directly to business value.
-->

---
layout: center
class: text-center
---

# How do we measure "better"?

<v-clicks>

<div class="text-2xl mt-10 opacity-90">

You can't just <span class="text-red-400">**vibe check**</span> AI improvements ü§∑

</div>

<div class="text-2xl mt-8 opacity-90">

You can't rely on <span class="text-orange-400">**gut feelings**</span> üé≤

</div>

<div class="text-3xl mt-12 text-green-400 font-bold">

You need <span class="text-cyan-400">üìä benchmarks</span> and <span class="text-purple-400">üìà metrics</span>

</div>

<div class="text-xl mt-10 opacity-75">

The only way to objectively track progress

</div>

</v-clicks>

<!--
Key point: Establish credibility early. Can't measure AI with vibes. Wait for clicks to build - let each point land. This sets up why the benchmarks coming next actually matter. Without objective metrics, we're just guessing.
-->

---
layout: center
class: text-center
---

# Would you upgrade your laptop to 2x performance for free?


<v-clicks>

But there are a few catches:



- ‚úÖ No budget approval needed
- ‚úÖ You don't need to wait for your "refresh cycle"
- ‚úÖ No cost increase

</v-clicks>

<v-click>

## Anyone interested?

</v-click>

<!--
Opening Hook (1 min): Make it relatable. Everyone wants free performance. Build anticipation with the catches - they expect a trick. Then reveal: there IS no catch. Same price, better model. Let that sink in before moving forward.
-->

---
layout: two-cols
---

# The Free Upgrade

<div class="text-6xl mt-10">
üí∞ $3 / $15
</div>

<div class="text-2xl mt-5 opacity-75">
Claude 3.5 Sonnet
</div>

::right::

<div class="mt-20"></div>

<v-click>
<div class="text-6xl mt-10">
üí∞ $3 / $15
</div>



<div class="text-2xl mt-5 opacity-75">
Claude 4.5 Sonnet
</div>

<div class="text-3xl mt-10 text-green-400">
~50+% better performance
</div>

</v-click>

<!--
The reveal: Same exact price. Pause after showing left side. Then click to show right side with performance boost. Let the audience process: free upgrade = no budget approval needed. This removes the biggest barrier to adoption.
-->

---

# The Easiest Performance Win

<div class="text-2xl mt-10">

With new models dropping every month, the biggest improvement you can make isn't:

</div>

<v-clicks>

- ‚ùå Rewriting your prompts
- ‚ùå Fine-tuning a custom model
- ‚ùå Adding more RAG context
- ‚ùå Implementing complex workflows

<div class="text-3xl mt-10 text-green-400">

‚úÖ **Just upgrade the model**

</div>

</v-clicks>

<v-click>

<div class="text-xl mt-10 opacity-75">

A couple minute code change. No budget approval. Instant 50%+ gains.

</div>

</v-click>

<!--
Critical message: Don't overcomplicate this. The biggest gains aren't from fancy prompt engineering or custom models. It's literally just changing one string in your code. Emphasize the simplicity - this is the lowest-effort, highest-impact change they can make.
-->

---
layout: center
class: text-center
---

# Sounds too good to be true?

Let me show you the benchmarks

<!--
Transition to proof (3 min section starts): They're skeptical. Good. Now back up every claim with data. This isn't marketing fluff - these are industry-standard benchmarks that can't be gamed. Prepare to show real numbers.
-->


---

# SWE-bench Verified
## Real-World Coding Performance

<!--
Blank slide - use this beat to transition tone from promises to proof. Take a breath. The next section is data-heavy, so signal the shift in your delivery.
-->



**What it measures:**
- Real GitHub issues and pull requests
- Tests ability to understand, plan, and fix actual software bugs
- Industry-standard for measuring coding capability

<v-clicks>

**Performance:**
- **Claude 3.5 Sonnet**: ~49%
- **Claude 4.5 Sonnet**: 77.2% (82% with parallel compute)
- **Improvement**: <span class="text-green-400 text-2xl font-bold">58% better</span>

</v-clicks>

<!--
Benchmark 1 (most important): SWE-bench uses REAL GitHub issues - not toy problems. Emphasize: real pull requests, real bugs, real codebases. 58% improvement means tasks that failed now succeed. Wait for clicks - let each number sink in. The 77% ‚Üí 82% shows it's still improving with better infrastructure.
-->

---

# The Rapid Evolution

| Timeframe | Model | Human Equivalent | Score |
|-----------|-------|------------------|-------|
| 2022 Mar | GPT-3.5 | ü§î Seen code, doesn't understand | <span class="text-red-400">~3%</span> |
| 2024 Mar | GPT-4 / Claude 3 | üë∂ Eager Intern | <span class="text-orange-400">~13-28%</span> |
| 2024 Jun |  [Claude Sonnet 3.5](https://www.anthropic.com/news/claude-3-5-sonnet) | üíº Junior dev | <span class="text-yellow-400">~33-49*%</span> |
| 2025 Feb | [Claude Sonnet 3.7](https://www.anthropic.com/news/claude-3-7-sonnet) | üéØ Mid-level | <span class="text-blue-400">~62%</span> |
| 2025 May | [Claude Sonnet 4](https://www.anthropic.com/news/claude-4)  | üöÄ Experienced Dev | <span class="text-cyan-400">~72%</span> |
| 2025 Oct | **[Claude Sonnet 4.5](https://www.anthropic.com/news/claude-sonnet-4-5) ** | **‚≠ê Senior+** | **<span class="text-green-400 text-xl">77.2%</span>** |

<div class="mt-8 text-xl text-center opacity-75">
That's <span class="text-green-400 font-bold">25x improvement</span> in solving real GitHub issues üöÄ
</div>

<style>
table {
  font-size: 0.85em;
}
</style>

<!--
The big picture: This table shows the INSANE pace of improvement. 3 years from "can't code" to "senior dev level." Point out: GPT-3.5 at 3% was only 3 years ago. We've had 25x improvement. Linear time, exponential progress. This isn't slowing down - if anything, it's accelerating.
-->

---

# Why Coding is the Best Benchmark

<div class="grid grid-cols-3 gap-12 mt-12 text-lg">

<div v-click class="space-y-4">

### üéØ Objectively<br/>Measurable

<div class="space-y-3 mt-6">

- <span class="text-green-400">‚úÖ Works</span> or <span class="text-red-400">‚ùå doesn't</span>
- Tests pass or fail
- <span class="text-cyan-400 font-bold">Zero ambiguity</span>
- No tricks allowed

</div>

</div>

<div v-click class="space-y-4">

### ü§î Other Tasks<br/>Fail

<div class="space-y-3 mt-6">

- <span class="opacity-50">"Product review"</span><br/>Sounds good, means nothing
- <span class="opacity-50">"Summarize doc"</span><br/>Hard to measure
- <span class="opacity-50">"Marketing copy"</span><br/>Persuasive ‚â† correct

</div>

</div>

<div v-click class="space-y-4">

### üß† Real<br/>Understanding

<div class="space-y-3 mt-6">

- Requirements, architecture, edge cases
- <span class="text-orange-400 font-bold">Can't fake it</span> üî®
- Compiler/tests ruthless
- Beyond <span class="text-green-400">works</span> vs <span class="text-red-400">broken</span>

</div>

</div>

</div>

<!--
Address the skeptic: Why trust these benchmarks? Because code either works or it doesn't. Tests pass or fail. No wiggle room, no subjective grading. Wait for each click - contrast objective coding tasks with fuzzy tasks like "write marketing copy." Code benchmarks can't be gamed or optimized for.
-->

---
layout: center
class: text-center
---

# Better Every Call = Compounding Wins

The math gets interesting fast when you consider multiple attempts vs "single shot"

<!--
Transition to advanced concept: Most people think linearly about improvements. But AI gets retried - debugging, iterations, agent loops. Better per-call means MUCH better over multiple attempts. Set up the math coming next.
-->

---

# The Compounding Effect

<div class="grid grid-cols-2 gap-8">
<div>

## Single Attempt

**Claude 3.5:**
- Chance of success: 49%
- Chance of failure: 51%

**Claude 4.5:**
- Chance of success: 77%
- Chance of failure: 23%

</div>
<div v-click>

## With 3 Attempts

**Claude 3.5:**
- Chance you solve it: **87.5%**
- (At least 1 of 3 succeeds)

**Claude 4.5:**
- Chance you solve it: **98.8%**
- (At least 1 of 3 succeeds)

<div class="text-green-400 text-2xl mt-5">
11.3% absolute improvement
</div>

</div>
</div>

<v-click>

<div class="mt-8 text-center text-xl">

**Real Impact:** Bug you'd retry 3 times goes from 87% ‚Üí 99% fix rate

</div>

</v-click>

<!--
The killer insight: With retries, 49% ‚Üí 77% single-shot becomes 87% ‚Üí 99% success rate. Walk through the math slowly. Show left side first (single attempt), then click to reveal right side (3 attempts). That 11% absolute improvement is MASSIVE - it's the difference between "usually works" and "almost always works." This is why better models compound in real workflows.
Math: 1 - (failure_rate^attempts). 3.5: 1-(0.51¬≥)=87.5%. 4.5: 1-(0.23¬≥)=98.8%
-->

---

# Switching Models: One Line of Code

```python {all|2|5|all}
import boto3
bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')

response = bedrock.invoke_model(
    modelId='anthropic.claude-sonnet-3-5-v2',  # Old model
    body=json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1024,
        "messages": [{"role": "user", "content": "Hello!"}]
    })
)
```

<v-click>

```python {5}
import boto3
bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')

response = bedrock.invoke_model(
    modelId='anthropic.claude-sonnet-4-5-v2',  # New model - that's it!
    body=json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1024,
        "messages": [{"role": "user", "content": "Hello!"}]
    })
)
```

</v-click>

<div v-click class="text-center mt-5 text-xl">

Change one string. Get 58% better performance. Same price.

</div>

<!--
The proof it's easy: Live code example. Highlight line 2 (old model), then click to show line 5 (new model). ONE STRING CHANGE. Same API, same parameters, same code structure. No refactoring, no migration guide, no breaking changes. Emphasize: this is literally all you need to change. If you have more complex code, it's still just this one modelId parameter.
-->

---

# OSWorld: Computer Use
## Autonomous Agent Capabilities

**What it measures:**
- Real computer tasks: navigating apps, editing files, multi-step workflows
- Tests autonomous agent capabilities
- Measures ability to use computers like humans do

<v-clicks>

**Performance:**
- **Claude 4.0 Sonnet**: 42.2% (4 months ago)
- **Claude 4.5 Sonnet**: 61.4%
- **Improvement**: <span class="text-green-400 text-2xl font-bold">45% better</span> in just 4 months

**Translation:**
- Can autonomously complete complex workflows
- Navigates UIs, runs commands, edits documents
- Best-in-class for agent applications

</v-clicks>

<!--
Benchmark 2 - Agentic capabilities: OSWorld tests autonomous computer use - navigating UIs, running commands, multi-step workflows. This matters for automation and agent applications. 45% improvement in just 4 months (4.0 to 4.5). If you're building agents or automation, this benchmark shows it can actually do the work autonomously now.
-->

---

<!--
Separator slide - pause for questions or take a breath. Transitioning from benchmarks to autonomous work capability.
-->

---
layout: center
class: text-center
---

# It can work for 30+ hours straight

Without losing focus or context

<!--
The attention-grabber: 30+ HOURS. Let that number land. No human can do this. Sets up the practical implications on next slide. This is where AI stops being a tool and becomes a tireless teammate.
-->

---

# Autonomous Work Duration

Here's the game-changer: how long can it stay focused without human intervention?

<v-clicks>

**Performance:**
- **Claude 4.0 Opus**: 7 hours of focused work
- **Claude 4.5 Sonnet**: <span class="text-green-400 text-4xl font-bold">30+ hours</span> of focused work
- **Improvement**: <span class="text-green-400 text-2xl font-bold">4x longer</span> autonomous operation

**What this means:**
- Start it Friday evening, review Monday morning
- Handles complex refactors while you sleep
- Fewer "I need to ask the human" interruptions

</v-clicks>

<div v-click class="mt-5 text-sm opacity-75">
*Requires proper feedback loops (tests, linting, etc.)
</div>

<!--
The game-changer metric: 7 hours ‚Üí 30+ hours is a 4x improvement in autonomous operation. Emphasize the practical applications: start Friday, review Monday. Complex refactors overnight. The asterisk matters - this requires good feedback loops (tests, linters). It's not magic, it's proper engineering + AI capability. Wait for clicks to build the story.
-->

---
layout: center
class: text-center
---

# Real Business Impact

What companies actually achieved with Claude 4.5 in production

<!--
Transition to business value (15 sec): Benchmarks are great, but business leaders want ROI. Now show what companies ACTUALLY achieved in production. These aren't cherry-picked examples - these are case studies from Anthropic's public customer stories.
-->

---
layout: two-cols
---

# Case Study 1
## HackerOne
### Security Engineering

**Company Context:**
- Bug bounty platform
- Processes thousands of vulnerability reports with "Hai" AI security agents
- Time-sensitive security work

::right::

<div class="mt-20"></div>

<v-clicks>

### Results

<div class="text-5xl text-green-400 mt-5">44%</div>
<div class="text-xl">reduction in vulnerability intake time</div>

<div class="text-5xl text-green-400 mt-5">25%</div>
<div class="text-xl">improvement in accuracy</div>

</v-clicks>

<!--
Case Study 1 - HackerOne (1 min): Bug bounty platform, time-sensitive security work. Read company context first to establish credibility. Then reveal results with clicks. 44% faster + 25% more accurate. In security, both matter enormously - speed reduces exposure window, accuracy reduces wasted effort on false positives. This is measurable business impact.
Source: https://www.anthropic.com/research/building-ai-cyber-defenders
-->

---
layout: center
---

# The HackerOne Impact

<div class="text-3xl mt-10">
44% faster + 25% more accurate
</div>

<div class="mt-8 text-xl">
= Security teams go from reactive to proactive
</div>

<v-click>

<div class="mt-10">

**What changed:**
- Process 2x more vulnerability reports with same team
- Critical patches deployed hours faster, not days
- Fewer false positives wasting engineer time

</div>

</v-click>

<div class="text-sm opacity-50 mt-10">
Source: Building AI for cyber defenders - Anthropic
</div>

<!--
Drill into the implications: What does 44% faster + 25% accurate actually MEAN? It means going from reactive (overwhelmed by volume) to proactive (ahead of threats). Wait for click, then spell it out: 2x volume, hours not days for patches, fewer false positives. Make it concrete and relatable.
-->

---
layout: two-cols
---

# Case Study 2
## Palo Alto Networks
### Developer Productivity

**Company Context:**
- World's largest cybersecurity company
- **2,500 developers** using Claude (‚Üí 3,500)
- Complex product integrations
- 30-35% of time in initial development

::right::

<div class="mt-16"></div>

<v-clicks>

### Results

<div class="text-4xl text-green-400 mt-5">20-30%</div>
<div class="text-lg">increase in feature velocity</div>

<div class="text-4xl text-green-400 mt-5">70%</div>
<div class="text-lg">faster onboarding for junior devs</div>

<div class="text-xl mt-5">Months ‚Üí Weeks</div>

</v-clicks>

<!--
Case Study 2 - Palo Alto (1m 15s): Massive scale - 2,500 developers (growing to 3,500). Read context: world's largest cybersecurity company, complex products. Then clicks for results: 20-30% faster feature velocity, 70% faster onboarding. That onboarding number is HUGE - junior devs productive in weeks not months. The scale proves this isn't a fluke.
Source: https://www.claude.com/customers/palo-alto-networks
-->

---
layout: center
---

# The Palo Alto Networks Impact

<div class="text-3xl mt-10">
2,500 developers. 20-30% faster.
</div>

<div class="mt-8 text-xl">
Junior devs onboard 70% faster
</div>

<v-click>

<div class="mt-10">

**Think about that:**
- If onboarding took 6 months ‚Üí now takes 2 months
- Junior devs solving mid-level problems on day one
- Features shipping weeks faster

</div>

</v-click>

<v-click>

<div class="mt-8 text-lg opacity-75">
Expanding to 3,500 devs because the ROI is undeniable
</div>

</v-click>

<div class="text-sm opacity-50 mt-10">
Source: Customer story | Palo Alto Networks | Claude
</div>

<!--
Make it relatable: Do the math with the audience. 6 months onboarding ‚Üí 2 months. Junior solving mid-level problems day one. Features weeks faster. First click shows immediate impact, second click shows they're EXPANDING because ROI is proven. When a company adds 1,000 more developer seats, you know it's working.
-->

---
layout: center
---

# Case Study 3
## IG Group
### Full ROI in 3 Months

<div class="grid grid-cols-2 gap-8 mt-10">
<div>

**Company Context:**
- Financial services
- Marketing and dev teams
- Speed-to-market critical

</div>
<div v-click>

**Results:**
- 2x productivity
- Triple-digit speed-to-market
- **ROI in 90 days**
- Reduced agency dependency

</div>
</div>

<div v-click class="text-center mt-10">

> "3 months to full ROI. After that, it's pure profit on the productivity gains."

</div>

<!--
Case Study 3 - IG Group (1 min): The ROI slide. Financial services, marketing + dev teams. Click to reveal results, then click for the money quote: "ROI in 90 days." That's the number finance teams care about. After 3 months, pure profit. Reduced agency dependency means cost savings AND faster execution. This addresses the "is it worth it" question directly.
-->

---

# Proven Across Industries

<div class="grid grid-cols-3 gap-8 mt-10 text-center">
<div v-click>

### Devin
<div class="text-sm opacity-75 mb-3">Coding Platform</div>

<div class="text-3xl text-green-400">+18%</div>
<div class="text-sm">planning performance</div>

</div>
<div v-click>

### Box AI
<div class="text-sm opacity-75 mb-3">Document Processing</div>

<div class="text-3xl text-green-400">67% ‚Üí 80%</div>
<div class="text-sm">image document accuracy</div>

</div>
<div v-click>

### Novo Nordisk
<div class="text-sm opacity-75 mb-3">Pharmaceutical</div>

<div class="text-3xl text-green-400">Dramatic</div>
<div class="text-sm">acceleration in regulatory docs</div>

</div>
</div>

<div v-click class="text-center mt-10 text-xl opacity-75">
From security to pharma, measurable results
</div>

<!--
Breadth proof (30 sec): Quick-fire examples to show this isn't limited to one industry. Wait for each click - coding (Devin), documents (Box), pharma (Novo). Different use cases, all seeing measurable gains. The pattern holds across industries. This builds confidence it will work for YOUR use case too.
-->

---
layout: center
class: text-center
---

# What This Means For Your Team

Let's bring this home

<!--
Transition to action (15 sec): Now make it personal. Stop talking about other companies. "What should YOU do?" This section is about removing barriers and giving concrete next steps they can take immediately.
-->

---

# What You Should Do Monday

<v-clicks>

### 1. Pick One Painful Task
That database migration you've been putting off? That massive refactor? Start there.

### 2. Set Up Your Success Metrics
Measure time saved, bugs caught, code quality. You need proof for the next quarterly review.

### 3. Start Small, Scale Fast
One team, one project, prove value. Then roll out based on results, not hope.

</v-clicks>

<!--
Actionable steps: Three concrete things they can do MONDAY (not "someday"). Click through each one. Pick one painful task (make it relatable - everyone has that migration they're avoiding). Set up metrics (you need proof for next quarter). Start small (reduce risk, prove value). Make each step feel achievable, not overwhelming.
-->

---
layout: center
class: text-center
---

# So Here's the Deal

<div class="text-4xl mt-10">

Same price. 58% better performance.

</div>

<v-click>

<div class="text-3xl mt-10">

Companies hit ROI in 90 days.

</div>

</v-click>

<v-click>

<div class="text-3xl mt-10 text-green-400">

Your competitors are already using it.

</div>

</v-click>

<v-click>

<div class="text-2xl mt-10">

The only question: **How fast can we move?**

</div>

</v-click>

<!--
The close (15 sec): Recap the key points as a decision framework. Same price (no budget barrier). 58% better (proven by benchmarks). 90-day ROI (proven by companies). Then the FOMO: competitors are already using it. End with urgency: "How fast can we move?" Make inaction feel riskier than action.
-->

---

# Questions & Answers

<div class="grid grid-cols-2 gap-8 mt-5 text-sm">
<div>

**"What's the catch?"**

We hit throttle limits on AWS shared accounts. Default: 200 req/min. Raised to 1000/min. Non-prod done 10/24, prod pending.

**"How long until 4.5 is outdated?"**

Performance improves, price stays same. Quick adopters win now, upgrade free later.

</div>
<div>

**"Do we need to retrain our team?"**

No. Drop-in replacement. Change one parameter.

**"What about hallucinations/accuracy?"**

25% accuracy improvement (HackerOne). 89% ‚Üí 98% safety score. Better at being correct.

</div>
</div>

<!--
Q&A preparation: These are the four most common objections/questions. Have answers ready. "Catch" = throttle limits (addressable). "Outdated" = upgrades keep coming (good thing). "Retrain" = no (removes barrier). "Hallucinations" = better accuracy (data-backed). Anticipate and address concerns proactively.
-->

---
layout: center
class: text-center
---

# Appendix

Additional benchmarks, sources, and technical details

<!--
Appendix intro: Backup slides for deep-dive questions. Don't present these unless asked. Have them ready for technical audience members who want more detail or sources to verify claims.
-->

---

# Additional Benchmarks

<div class="grid grid-cols-2 gap-8 mt-10">
<div>

### AIME 2025
**Advanced Math**

- With Python tools: **100%**
- Without Python tools: 87%

### GPQA Diamond
**Science Reasoning**

- Score: **83.4%**

</div>
<div>

### Response Quality

- Harmless response rate: **99.29%**
- Over-refusal rate: **0.02%** (down from 0.15%)

### Official Sources

- Launch: September 29, 2025
- API: `claude-sonnet-4-5`
- Available: Amazon Bedrock, Claude.ai, Claude Code

</div>
</div>

<!--
Additional benchmarks backup: More proof points if anyone questions the claims. AIME shows math reasoning (100% with tools!). GPQA shows science expertise. Response quality shows safety improvements (fewer refusals, higher accuracy). Only go here if someone asks for more benchmarks or challenges the claims.
-->

---

# Follow-Up Resources

**To Share After Presentation:**

1. [Anthropic official announcement](https://www.anthropic.com/news/claude-sonnet-4-5)
2. [API documentation](https://docs.claude.com/)
3. Case studies PDF (HackerOne, Palo Alto, IG Group)
4. Internal pilot team signup sheet
5. Baseline metrics template

**For Technical Deep-Dive:**

- SWE-bench methodology and results
- API migration guide (3.5 ‚Üí 4.5)
- Context window optimization strategies
- [Prompt engineering best practices](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices)

<!--
Resources slide: Have this ready to share after. Send the link to this deck or a PDF with these resources. The case studies PDF and internal pilot signup are key - make adoption easy. Mention you'll email these out so they don't need to screenshot.
-->

---
layout: center
class: text-center
---

# Thank You

Questions?

<!--
Final slide: Leave this up during Q&A. End on a friendly, open note. You've made the case - now let them process and ask questions. Be ready to jump to appendix slides if needed.
-->

<div class="text-sm opacity-50 mt-10">
Created: 2025-10-29 | Framework: "The No-Brainer Upgrade" | Version 1.0
</div>
